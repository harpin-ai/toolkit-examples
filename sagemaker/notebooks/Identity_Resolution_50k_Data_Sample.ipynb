{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harpin AI Identity Resolution - 50k Data Sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites\n",
    "1. **Note**: This notebook contains elements which render correctly in Jupyter interface. Open this notebook from an Amazon SageMaker Notebook Instance or Amazon SageMaker Studio.\n",
    "1. Ensure that IAM role used has **AmazonSageMakerFullAccess**.\n",
    "1. Some hands-on experience using [Amazon SageMaker](https://aws.amazon.com/sagemaker/).\n",
    "1. To use this algorithm successfully, ensure that:\n",
    "    1. Either your IAM role has these three permissions and you have authority to make AWS Marketplace subscriptions in the AWS account used:\n",
    "        1. **aws-marketplace:ViewSubscriptions**\n",
    "        1. **aws-marketplace:Unsubscribe**\n",
    "        1. **aws-marketplace:Subscribe**\n",
    "    1. or your AWS account has already subscribed to this free product from AWS Marketplace: [Identity Resolution](https://aws.amazon.com/marketplace/pp/prodview-etnavzupbnthk?sr=0-7&ref_=beagle&applicationId=AWSMPContessa)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Amazon SageMaker environment\n",
    "\n",
    "The session remembers our connection parameters to Amazon SageMaker. We'll use it to perform all of our Amazon SageMaker operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up your S3 bucket\n",
    "s3_bucket = 'YOUR S3 BUCKET'\n",
    "#Set up the Algorithm ARN from your algorithm subscription\n",
    "algorithm_arn = 'YOUR ALGORITHM ARN'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Data Description\n",
    "For this example, the input dataset contains 50,000 rows of sample identity data in a CSV format.  The file is located in the Git repository that was cloned into this notebook.",
    "Other notebooks will provide examples of loading data from different sources, such as AWS S3.  Data can be read from files in CSV, Avro, or Parquet format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "common_prefix = '/harpin/batch_resolution'\n",
    "common_prefix_url = 's3://' + s3_bucket + common_prefix\n",
    "#Set up the file location for the sample data\n",
    "input_data = common_prefix + '../data/sample_data_50k'\n",
    "\n",
    "#Upload the configuration file and sample data to S3.\n",
    "config_local = '../config/sample_data_50k.yml'\n",
    "data_local = '../data/sample_data_50k'\n",
    "config_prefix = common_prefix + '/sample_data_50k/config'\n",
    "data_prefix = common_prefix + '/sample_data_50k/data'\n",
    "source_config = sagemaker_session.upload_data(config_local, bucket=s3_bucket, key_prefix=config_prefix)\n",
    "sagemaker_session.upload_data(data_local, bucket=s3_bucket, key_prefix=data_prefix)\n",
    "\n",
    "#Set up the output s3 location for the identity graph\n",
    "identity_graph = common_prefix\n",
    "print('Input identity data location: ', data_prefix)\n",
    "print('Source config file location: ', config_prefix)\n",
    "print('Output identity graph location: ' + identity_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Identity Resolution SageMaker TrainingJob using Algorithm ARN\n",
    "\n",
    "We will use the tools provided by the Amazon SageMaker Python SDK to create the [AlgorithmEstimator](https://sagemaker.readthedocs.io/en/stable/api/training/algorithm.html) to perform the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.algorithm import AlgorithmEstimator\n",
    "\n",
    "algo = AlgorithmEstimator(\n",
    "    algorithm_arn=algorithm_arn,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.2xlarge',\n",
    "    base_job_name='harpin-ai-identity-resolution-50k-sample',\n",
    "    output_path=identity_graph\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Identity Resolution Clustering with SageMaker TrainingJob\n",
    "Note that the TrainingJob actually performs a clustering process. The clustering process produces an identity graph, which clusters the records in the input dataset into a set of dis-joint customer profiles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify the input data sources for up to 3 channels (i.e. clustering, clustering2 and clustering3), and a channel config file.\n",
    "#And run the identity resolution process by calling the fit() method\n",
    "print('Now run the identity resolution clustering using Algorithm ARN %s in region %s' % (algorithm_arn, sagemaker_session.boto_region_name))\n",
    "algo.fit({\"sample_data_50k\": input_data, \n",
    "          \"channel_config\": source_config})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identity Graph Data and Format\n",
    "The identity graph will be stored in a folder with one or more files with the exact same type as the input files (csv, avro or parquet). If the input files are CSVs, then the output will contains CSV files too. All the fields in the input files will be retained in the output files, along with one additional field called PIN. The field PIN is the assigned unique customer profile identitfier. Customer records with the same (non-default) PIN are considered to be referring to the same customer profile. The default value for PIN is -1, meaning that there is not enough information available in the input record to determine which customer profile it belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here is the output path for storing the results from running the algorithm\n",
    "path = algo.output_path\n",
    "!aws s3 ls $path/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make sure that you change the value to match for your \"specific_run\".\n",
    "specific_run = 'harpin-ai-identity-resolution-50k-sample-TIMESTAMP'\n",
    "\n",
    "#Specify a temporary directory, and extract the identity graph from S3 to the temp_data directory for analysis\n",
    "temp_data = './temp_data'\n",
    "!rm -rf $temp_data\n",
    "!mkdir -p $temp_data\n",
    "!aws s3 cp $path/$specific_run/output/output.tar.gz $temp_data/\n",
    "!tar -xzvf $temp_data/output.tar.gz -C $temp_data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identity Graph Analysis\n",
    "Now the identity resolution clustering process is finished and we have the identity graph. We can perform some simple analysis on the identity graph such as record count, unique customer profiles, duplicate identity analysis, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the pyspark libraries and create the spark object\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Load the identity graph into spark dataframe in csv\n",
    "identity_graph = spark.read.format('csv').options(header='true', inferSchema='false', delimiter='|') \\\n",
    "                                         .option('mode', 'DROPMALFORMED') \\\n",
    "                      .load(temp_data + '/identity_graph/')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List the fields and their data types in the identity graph\n",
    "#The identity graph will contain the union of fields from all the input data sources, plus an additional field \"pin\"\n",
    "identity_graph.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the number of records in the identity graph\n",
    "identity_graph.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the unique number of customer profiles in the identity graph\n",
    "identity_graph.filter(F.col('pin') != '-1') \\\n",
    "              .select('pin') \\\n",
    "              .distinct() \\\n",
    "              .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform duplicate records analysis for the identity graph. For example, there are 47 records which are assigned the same PIN (10000000543). \n",
    "#Those records are considered to be referring to the same customer.\n",
    "identity_graph.groupBy('pin') \\\n",
    "              .count() \\\n",
    "              .orderBy(F.desc('count')) \\\n",
    "              .show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean up the temporary directory\n",
    "!rm -rf $temp_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
