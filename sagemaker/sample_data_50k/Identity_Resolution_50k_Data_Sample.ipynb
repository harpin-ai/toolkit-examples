{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harpin AI Identity Resolution - 50k Data Sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites\n",
    "1. **Note**: This notebook contains elements which render correctly in Jupyter interface. Open this notebook from an Amazon SageMaker Notebook Instance or Amazon SageMaker Studio.\n",
    "1. Ensure that IAM role used has **AmazonSageMakerFullAccess**.\n",
    "1. Some hands-on experience using [Amazon SageMaker](https://aws.amazon.com/sagemaker/).\n",
    "1. To use this algorithm successfully, ensure that:\n",
    "    1. Either your IAM role has these three permissions and you have authority to make AWS Marketplace subscriptions in the AWS account used:\n",
    "        1. **aws-marketplace:ViewSubscriptions**\n",
    "        1. **aws-marketplace:Unsubscribe**\n",
    "        1. **aws-marketplace:Subscribe**\n",
    "    1. or your AWS account has already subscribed to this free product from AWS Marketplace: [Identity Resolution](https://aws.amazon.com/marketplace/pp/prodview-etnavzupbnthk?sr=0-7&ref_=beagle&applicationId=AWSMPContessa)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Amazon SageMaker environment\n",
    "\n",
    "The sagemaker session remembers our connection parameters to Amazon SageMaker. We'll use it to perform all of our Amazon SageMaker operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "role = sagemaker.get_execution_role()\n",
    "sagemaker_session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up your s3 bucket, or use the SageMaker default s3 bucket\n",
    "s3_bucket = 'YOUR S3 BUCKET'\n",
    "#s3_bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "#Set up the Algorithm ARN from your algorithm subscription\n",
    "algorithm_arn = 'YOUR ALGORITHM ARN'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Data Description\n",
    "For this example, the input dataset contains 50,000 rows of sample identity data in a CSV format.  The file is located in the Git repository that was cloned into this notebook.Other notebooks will provide examples of loading data from different sources, such as AWS S3.  Data can be read from files in CSV, Avro, or Parquet format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up the common prefix\n",
    "common_prefix = '/harpin/batch-resolution/sample-data-50k'\n",
    "\n",
    "#Upload the configuration file (channel_config.yml) and sample data to S3.\n",
    "config_local = 'channel_config.yml'\n",
    "data_local = 'data'\n",
    "config_prefix = common_prefix + '/config'\n",
    "data_prefix = common_prefix + '/data'\n",
    "source_config = sagemaker_session.upload_data(config_local, bucket=s3_bucket, key_prefix=config_prefix)\n",
    "clustering_data = sagemaker_session.upload_data(data_local, bucket=s3_bucket, key_prefix=data_prefix)\n",
    "\n",
    "#Set up the output s3 location for the identity graph\n",
    "identity_graph = 's3://' + s3_bucket + '/' + common_prefix + '/identity-graph'\n",
    "print('Input identity data location: ', clustering_data)\n",
    "print('Source config file location: ', source_config)\n",
    "print('Output identity graph location: ' + identity_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Identity Resolution SageMaker TrainingJob using Algorithm ARN\n",
    "\n",
    "We will use the tools provided by the Amazon SageMaker Python SDK to create the [AlgorithmEstimator](https://sagemaker.readthedocs.io/en/stable/api/training/algorithm.html) to perform the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.algorithm import AlgorithmEstimator\n",
    "\n",
    "algo = AlgorithmEstimator(\n",
    "    algorithm_arn=algorithm_arn,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.2xlarge',\n",
    "    base_job_name='harpin-ai-identity-resolution-50k-sample',\n",
    "    output_path=identity_graph\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Identity Resolution Clustering with SageMaker TrainingJob\n",
    "Note that the TrainingJob actually performs a clustering process. The clustering process produces an identity graph, which clusters the records in the input dataset into a set of dis-joint customer profiles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: harpin-ai-identity-resolution-50k-sampl-2024-09-30-19-31-49-222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now run the identity resolution clustering using Algorithm ARN arn:aws:sagemaker:us-west-2:594846645681:algorithm/identity-resolution13-1d953864e3d2303e932e639006777330 in region us-west-2\n",
      "2024-09-30 19:31:50 Starting - Starting the training job...\n",
      "2024-09-30 19:32:05 Starting - Preparing the instances for training...\n",
      "2024-09-30 19:32:38 Downloading - Downloading the training image...\n",
      "2024-09-30 19:33:04 Training - Training image download completed. Training in progress.\u001b[34m24/09/30 19:33:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\u001b[0m\n",
      "\u001b[34mSetting default log level to \"WARN\".\u001b[0m\n",
      "\u001b[34mTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\u001b[0m\n",
      "\u001b[34m24/09/30 19:33:15 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\u001b[0m\n",
      "\u001b[34mStarting the identity resolution clustering process...\u001b[0m\n",
      "\u001b[34mTotal number of records from all data sources:  50000\u001b[0m\n",
      "\u001b[34mTotal number of records in the final identity graph: 50000\u001b[0m\n",
      "\u001b[34mFinished writing the identity_graph.\u001b[0m\n",
      "\u001b[34mIdentity resolution clustering process completed successfully.\u001b[0m\n",
      "\n",
      "2024-09-30 19:39:53 Uploading - Uploading generated training model\n",
      "2024-09-30 19:39:53 Completed - Training job completed\n",
      "Training seconds: 446\n",
      "Billable seconds: 446\n"
     ]
    }
   ],
   "source": [
    "#Specify the input data sources for up to 3 channels (i.e. clustering, clustering2 and clustering3), and a channel config file.\n",
    "#And run the identity resolution process by calling the fit() method\n",
    "print('Now run the identity resolution clustering using Algorithm ARN %s in region %s' % (algorithm_arn, sagemaker_session.boto_region_name))\n",
    "algo.fit({\"clustering\": clustering_data, \n",
    "          \"channel_config\": source_config})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identity Graph Data and Format\n",
    "The identity graph will be stored in a folder with one or more files with the exact same type as the input files (csv, avro or parquet). If the input files are CSVs, then the output will contains CSV files too. All the fields in the input files will be retained in the output files, along with one additional field called PIN. The field PIN is the assigned unique customer profile identitfier. Customer records with the same (non-default) PIN are considered to be referring to the same customer profile. The default value for PIN is -1, meaning that there is not enough information available in the input record to determine which customer profile it belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE harpin-ai-identity-resolution-50k-sampl-2024-09-30-19-04-31-044/\n",
      "                           PRE harpin-ai-identity-resolution-50k-sampl-2024-09-30-19-31-49-222/\n"
     ]
    }
   ],
   "source": [
    "#Here is the output path for storing the results from running the algorithm\n",
    "path = algo.output_path\n",
    "!aws s3 ls $path/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make sure that you change the value to match for your \"specific_run\".\n",
    "specific_run = 'harpin-ai-identity-resolution-50k-sample-TIMESTAMP'\n",
    "\n",
    "#Specify a temporary directory, and extract the identity graph from S3 to the temp_data directory for analysis\n",
    "temp_data = './temp_data'\n",
    "!rm -rf $temp_data\n",
    "!mkdir -p $temp_data\n",
    "!aws s3 cp $path/$specific_run/output/output.tar.gz $temp_data/\n",
    "!tar -xzvf $temp_data/output.tar.gz -C $temp_data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identity Graph Analysis\n",
    "Now the identity resolution clustering process is finished and we have the identity graph. We can perform some simple analysis on the identity graph such as record count, unique customer profiles, duplicate identity analysis, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/09/30 19:49:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Import the pyspark libraries and create the spark object\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the identity graph into spark dataframe in csv\n",
    "identity_graph = spark.read.format('csv').options(header='true', inferSchema='false', delimiter=',') \\\n",
    "                                         .option('mode', 'DROPMALFORMED') \\\n",
    "                      .load(temp_data + '/identity_graph/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('rid', 'string'),\n",
       " ('given_name', 'string'),\n",
       " ('middle_name', 'string'),\n",
       " ('sur_name', 'string'),\n",
       " ('dob', 'string'),\n",
       " ('email', 'string'),\n",
       " ('street_address', 'string'),\n",
       " ('city', 'string'),\n",
       " ('zip', 'string'),\n",
       " ('state', 'string'),\n",
       " ('country', 'string'),\n",
       " ('phone', 'string'),\n",
       " ('gender', 'string'),\n",
       " ('email2', 'string'),\n",
       " ('phone2', 'string'),\n",
       " ('ip_address', 'string'),\n",
       " ('loyalty_id', 'string'),\n",
       " ('pin', 'string')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#List the fields and their data types in the identity graph\n",
    "#The identity graph will contain the union of fields from all the input data sources, plus an additional field \"pin\"\n",
    "identity_graph.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Count the number of records in the identity graph\n",
    "identity_graph.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "26258"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Count the unique number of customer profiles in the identity graph\n",
    "identity_graph.filter(F.col('pin') != '-1') \\\n",
    "              .select('pin') \\\n",
    "              .distinct() \\\n",
    "              .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|        pin|count|\n",
      "+-----------+-----+\n",
      "|         -1|  833|\n",
      "|10000000129|    9|\n",
      "|10000005343|    8|\n",
      "|10000003761|    8|\n",
      "|10000002045|    8|\n",
      "|10000009889|    8|\n",
      "|10000003635|    8|\n",
      "|10000003935|    8|\n",
      "|10000009518|    8|\n",
      "|10000001604|    8|\n",
      "|10000005128|    7|\n",
      "|10000010153|    7|\n",
      "|10000006268|    7|\n",
      "|10000006251|    7|\n",
      "|10000001798|    7|\n",
      "|10000005264|    7|\n",
      "|10000012096|    7|\n",
      "|10000012324|    7|\n",
      "|10000000566|    7|\n",
      "|10000011963|    7|\n",
      "+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Perform duplicate records analysis for the identity graph. For example, there are 47 records which are assigned the same PIN (10000000543). \n",
    "#Those records are considered to be referring to the same customer.\n",
    "identity_graph.groupBy('pin') \\\n",
    "              .count() \\\n",
    "              .orderBy(F.desc('count')) \\\n",
    "              .show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean up the temporary directory\n",
    "!rm -rf $temp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
